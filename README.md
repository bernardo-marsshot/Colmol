
# COLMOL ‚Äî Prot√≥tipo Django (Rece√ß√£o Inteligente)

Prot√≥tipo simples para **matching** entre **Nota de Encomenda (PO)** e **Guia de Remessa/Fatura** com:
- Upload de PDFs/Imagens (stub de OCR)
- Extra√ß√£o e normaliza√ß√£o (simulada)
- Mapeamento **c√≥digo fornecedor ‚Üî SKU interno**
- Valida√ß√£o de quantidades (com toler√¢ncia)
- **Certifica√ß√£o digital** (hash) por rece√ß√£o
- **Gest√£o por exce√ß√µes**
- **Dashboard** com KPIs e lista de documentos
- Admin Django para manuten√ß√£o de cat√°logos e regras

> ‚ö†Ô∏è Este prot√≥tipo usa um **OCR falso (stub)** para demonstra√ß√£o. Em produ√ß√£o, ligar a AWS Textract / Google Document AI / Azure Form Recognizer.

## Como correr
```bash
python -m venv .venv && source .venv/bin/activate  # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt
python manage.py migrate
python manage.py createsuperuser
python manage.py load_demo   # cria fornecedor, PO, linhas e mappings
python manage.py runserver
```

## Como testar rapidamente
1. Entre em **/admin** e confirme os dados de demonstra√ß√£o.
2. V√° a **/upload** e carregue um PDF/Imagem qualquer (o OCR √© simulado e preenche duas linhas).
3. Veja o **matching** na p√°gina do documento, incluindo exce√ß√µes e certifica√ß√£o.
4. Explore o **Dashboard** inicial em `/`.

## Pr√≥ximos passos
- Trocar o `fake_ocr_extract` por integra√ß√£o real (Textract/Document AI).
- Conector de email IMAP para ingest√£o autom√°tica.
- PWA m√≥vel para doca/QR e confer√™ncia f√≠sica.
- Export para PHC (CSV/Excel ou API/ODBC).
- KPIs por fornecedor e ranking de qualidade documental.

---

# üöÄ Ollama Proxy Server (Novo)

Proxy HTTP Express para Ollama com streaming completo, timeouts altos (>60s), e sele√ß√£o autom√°tica de modelos.

## Caracter√≠sticas

- ‚úÖ **Streaming completo** sem buffer (SSE/chunked)
- ‚úÖ **Timeouts altos**: keepAlive=610s, headers=620s (resolve timeout de 60s)
- ‚úÖ **Sele√ß√£o autom√°tica**: cheap_text (texto) vs vision (imagens)
- ‚úÖ **Upload de imagens** via multipart/form-data
- ‚úÖ **CPU-only otimizado**: OLLAMA_NUM_PARALLEL=1, mmap ativado
- ‚úÖ **CORS configurado** para localhost e Replit

## Endpoints

### `GET /health`
Verifica status do Ollama.

```bash
curl http://localhost:3000/health
# Response: {"status":"ok","ollama":"ready","models":1}
```

### `POST /generate`

**Texto simples (modelo cheap_text):**
```bash
curl -N http://localhost:3000/generate \
  -H "content-type: application/json" \
  -d '{"prompt":"Diz ol√° em 10 palavras.","stream":true}'
```

**Com imagem (modelo vision):**
```bash
curl -N http://localhost:3000/generate \
  -F "prompt=Descreve esta imagem." \
  -F "image=@exemplo.jpg"
```

## Configura√ß√£o (models.json)

```json
{
  "cheap_text": {
    "model": "llava:latest",
    "options": { "num_ctx": 1024, "batch_size": 256 }
  },
  "vision": {
    "model": "llava:latest",
    "options": { "num_ctx": 2048, "batch_size": 256 }
  }
}
```

## Iniciar Sistema Completo

```bash
bash start.sh
```

Isso inicia:
1. Ollama Server (porta 8000) - CPU-only, parallel=1
2. Node.js Proxy (porta 3000) - timeouts altos
3. Django App (porta 5000) - aplica√ß√£o principal

## Otimiza√ß√£o de Custos (CPU)

- Modelos quantizados (Q4_0, Q3_K_M)
- `num_ctx` baixo (1024-2048)
- `OLLAMA_NUM_PARALLEL=1`
- mmap ativado (sem `--no-mmap`)
- Modelo cheap_text para texto, vision s√≥ quando necess√°rio
