Objetivo:
Corrigir timeouts de 60s no endpoint /api/generate do meu projeto com Ollama em Replit (Nix), mantendo custo mínimo (CPU-only). Quero streaming de tokens, mmap ativado, concorrência=1, e um proxy HTTP com timeouts altos e suporte a SSE/chunked.

Contexto técnico (logs relevantes):

Ollama 0.9.5 a ouvir em 0.0.0.0:8000.

“no compatible GPUs were discovered” → CPU-only.

Runner arrancado com --no-mmap, --parallel 2 → lento em CPU.

Erro recorrente: 500 | 1m0s | POST "/api/generate" e context canceled após ~60s.

Preciso suporte a imagem opcional (modelo multimodal tipo LLaVA/liuhaotian) mas, quando não for necessário, usar modelo pequeno de texto para poupar CPU.

O que pretendo que cries/alteres

Configuração Nix (replit.nix)

Usa pkgs.ollama estável (ou o que já está na Nix store do Replit), nodejs LTS e pm2 para orquestrar.

Gera um replit.nix minimal com estes pacotes disponíveis.

Runner + Proxy com timeouts altos e streaming

Cria um servidor Node.js (Express) em server.js que faz proxy para http://127.0.0.1:8000/api/generate e retransmite em streaming (SSE/chunked) para o cliente.

Define timeouts: server.keepAliveTimeout = 610000, server.headersTimeout = 620000, e req.setTimeout(0)/agent.keepAlive para evitar cortar aos 60s.

Endpoint a expor: POST /generate com payload igual ao do Ollama, mas força stream: true se o cliente não enviar.

Suporta upload de imagem opcional (multipart/form-data) e envia para Ollama em images: ["data:image/...;base64,...."].

Arranque do Ollama sem --no-mmap, CPU-only e concorrência=1

Script start.sh:

Exporta variáveis:

export OLLAMA_HOST=0.0.0.0:8000
export OLLAMA_NUM_PARALLEL=1
export OLLAMA_MAX_QUEUE=64
export OLLAMA_KEEP_ALIVE=5m


Arranca ollama serve em background (com mmap por omissão — não usar --no-mmap).

Aguarda a porta 8000 responder.

Arranca node server.js.

Gestão de processos

Adiciona pm2 ou usa bash start.sh diretamente. Simplicidade > complexidade.

.replit deve usar:

run = "bash start.sh"


Modelos e opções para reduzir custos

Adiciona um ficheiro models.json com duas “presets”:

cheap_text: um modelo pequeno de texto (ex.: mistral:7b-instruct-q4_0 ou similar disponível) com opções padrão:

{ "model": "mistral:7b-instruct-q4_0", "options": { "num_ctx": 1024, "batch_size": 256 } }


vision: o modelo multimodal que já tenho (ex.: liuhaotian / LLaVA) só quando envio imagem:

{ "model": "liuhaotian", "options": { "num_ctx": 2048, "batch_size": 256 } }


O server.js deve detetar automaticamente: se o payload incluir imagem → usar vision; caso contrário → usar cheap_text, a menos que o cliente especifique model explicitamente.

Dependências (package.json)

Instala express, http-proxy, busboy (para multipart), e node-fetch ou undici.

Sem libs pesadas desnecessárias.

Segurança e CORS

Ativa CORS apenas para o meu domínio/localhost.

Limita o tamanho de upload (ex.: 5–10 MB).

Verificações e testes

Adiciona um endpoint GET /health que verifica http://127.0.0.1:8000/api/tags.

Fornece 2 curl de teste:

Texto rápido e barato (stream):

curl -N http://localhost:3000/generate \
  -H "content-type: application/json" \
  -d '{"prompt":"Diz olá em 10 palavras.","stream":true}'


Vision (com imagem):

curl -N http://localhost:3000/generate \
  -F "prompt=Resume esta imagem em 1 frase." \
  -F "image=@exemplo.jpg"


Poupança de custos no Replit

Garante que:

Sem Always-On (o workspace pode hibernar).

Logs verbosos desligados.

Modelos: preferir Q4_0 ou mais agressivo (Q3_K_M) para CPU.

OLLAMA_NUM_PARALLEL=1.

num_ctx baixo (1024–2048) por defeito.

Só carregar o modelo vision quando necessário.

Ficheiros que quero prontos (com conteúdo)
replit.nix

Node LTS, pm2 (opcional), curl, jq, ollama.

.replit
hidden = ["node_modules", ".pm2"]
run = "bash start.sh"

start.sh

Marca como executável.

Exporta env vars acima, arranca ollama serve (background), espera a porta 8000, e arranca node server.js na porta 3000.

package.json

type: module, scripts: "start": "node server.js".

server.js (Express)

POST /generate:

Se Content-Type: application/json: lê {prompt, images?, model?, stream?, options?}.

Se multipart: extrai prompt e 1 imagem, converte para base64.

Se não vier model: usa cheap_text se sem imagem, senão vision.

Garante stream: true se não vier.

Faz fetch para http://127.0.0.1:8000/api/generate e retransmite os chunks imediatamente ao cliente (SSE ou chunked, sem buffer).

Seta timeouts elevados no servidor HTTP (keepAliveTimeout/headersTimeout).

GET /health: verifica Ollama e devolve { status: "ok" } quando operacional.

CORS: permitir http://localhost:* e o meu domínio.

models.json

Conteúdo com as duas presets mencionadas.

(Opcional) README.md

Como testar com curl, e boas práticas de custo baixo.

Critérios de aceitação

Sem erro 500 | 1m0s e sem context canceled durante geração longa; respostas chegam em streaming.

GET /health devolve ok quando Ollama está pronto.

POST /generate com texto simples responde em < 3s com os primeiros tokens, em CPU.

POST /generate com imagem funciona e só usa o modelo vision quando há imagem.

Processo inicia com bash start.sh via .replit e não usa --no-mmap.

OLLAMA_NUM_PARALLEL=1 efetivo (confirmável por logs).

Configuração fácil de hibernar (sem Always-On) e sem logs excessivos.

Nota: Se o pacote ollama via Nix não estiver disponível na tua imagem atual do Replit, adapta o replit.nix para a channel correta ou faz curl https://ollama.ai/install.sh | sh no start.sh. Prefiro Nix se possível.
Implementa tudo e mostra-me os ficheiros criados/alterados.